seed: &seed 185

# Data Setting
data_folder:  data_dir
output_folder:  results/diffusion_unconditional_bert_transformer/
hub_cache_dir: &hub_cache_dir cache_dir/
ae_model_checkoint: results/transformer_vae/185/save/model.ckp


train_batch_size : &train_batch_size 16
valid_batch_size : &valid_batch_size 8
test_batch_size : &test_batch_size 8

train_csv:  train-clean-100.csv
valid_csv:  dev-clean.csv
test_csv:  test-clean.csv

train_loader_kwargs:
    batch_size: *train_batch_size
    shuffle: True,
    num_workers: 6

valid_loader_kwargs:
    batch_size: *valid_batch_size
    shuffle: False,
    num_workers: 6

test_loader_kwargs:
    batch_size: *test_batch_size
    shuffle: False,
    num_workers: 6

tokenizer_type: bert #gpt, char, bert
tokenizer:
    src: bert-base-uncased
    cache_dir: *hub_cache_dir

variational_beta : 1


number_of_epochs: 5
lr: 1e-3
print_every: 100
loss: mse #kl_div
freeze_embedding: &freeze_embedding False

# Parameters for decoding text
max_sequence_length: 100
temp: 0.9
decoder_search: temp_sample

ae_model_type : bert  #rnn, transformer or transformer_with_attention, bert
diff_model_type: transformer # transformer,unet_1d
embedding_size : &embedding_size 768
latent_size: &latent_size 16


ae_model:
    src: bert-base-uncased
    cache_dir: *hub_cache_dir
    masking_ratios: 0.0
    freeze_encoder: True
    freeze_decoder: True



diffusion:
    embedding_size: *embedding_size

# Transformer
diffusion_model:
    embedding_size: *embedding_size
    hidden_dim: 1024
    num_layers: 16
    nhead: 8
    dropout: 0.1
    bidirectional: True

# # Unet
# diffusion_model:
#      c_in: 1
#      c_out: 1
#      time_dim: 256





optimizer: AdamW # SGD , Adam, AdamW

train_logs: train_log.txt
test_wer: test_wer.txt




