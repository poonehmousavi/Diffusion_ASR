seed: &seed 185

# Data Setting
data_folder:  data_dir
output_folder:  results/rnn_vae/
hub_cache_dir: &hub_cache_dir cache_dir/

train_batch_size : &train_batch_size 16
valid_batch_size : &valid_batch_size 8
test_batch_size : &test_batch_size 8

train_csv:  train-clean-100.csv
valid_csv:  dev-clean.csv
test_csv:  test-clean.csv

train_loader_kwargs:
    batch_size: *train_batch_size
    shuffle: True,
    num_workers: 6

valid_loader_kwargs:
    batch_size: *valid_batch_size
    shuffle: False,
    num_workers: 6

test_loader_kwargs:
    batch_size: *test_batch_size
    shuffle: False,
    num_workers: 6

tokenizer_type: gpt #gpt, char, bert
tokenizer:
    src: gpt2
    cache_dir: *hub_cache_dir

variational_beta : 1


number_of_epochs: 5
lr: 1e-3
print_every: 100

freeze_embedding: &freeze_embedding False

# vocab_size : &vocab_size 1003 #should macch the embedding model for GPT2" 50257, Bert:30522
# tie_embeddings: &tie_embeddings False
# word_dropout: &word_dropout 0.0
# embedding_dropout: &embedding_dropout 0.5
# max_sequence_length: &max_sequence_length 50

model_type : rnn  #rnn, transformer or transformer_with_attention

model:
    embedding_size: 768
    hidden_size:  256
    dropout: 0.1
    latent_size: 16
    num_layers: 1
    tie_embedding: True
    word_dropout_rate : 0.5
    hidden_mode : mean



optimizer: adam # SGD , Adam

train_logs: train_log.txt
valid_wer: valid_wer.txt
test_wer: test_wer.txt





